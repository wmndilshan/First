{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wmndilshan/First/blob/master/convnext_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets\n",
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVsNWIgBXMAu",
        "outputId": "2fdbf7bc-271b-4922-fc19-23d11f0c43a4"
      },
      "id": "BVsNWIgBXMAu",
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opendatasets in /usr/local/lib/python3.11/dist-packages (0.1.22)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from opendatasets) (4.67.1)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (from opendatasets) (1.6.17)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from opendatasets) (8.1.8)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.32.3)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle->opendatasets) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle->opendatasets) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle->opendatasets) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle->opendatasets) (3.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYfTQu8RdLN9",
        "outputId": "1b0c402f-9bf8-44f2-ef42-d406084cc685"
      },
      "id": "TYfTQu8RdLN9",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (1.26.4)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.5.1+cu124)\n",
            "Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.1.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.12.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.17.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "od.download(\"https://www.kaggle.com/competitions/detect-ai-vs-human-generated-images\")\n",
        "od.download(\"https://www.kaggle.com/datasets/alessandrasala79/ai-vs-human-generated-dataset\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iq_Ihc2JXf9n",
        "outputId": "c9e44afe-8fe9-4634-ae1b-dddcad9ceab8"
      },
      "id": "iq_Ihc2JXf9n",
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./detect-ai-vs-human-generated-images\" (use force=True to force download)\n",
            "Please provide your Kaggle credentials to download this dataset. Learn more: http://bit.ly/kaggle-creds\n",
            "Your Kaggle username: upsddulaj\n",
            "Your Kaggle Key: ··········\n",
            "Dataset URL: https://www.kaggle.com/datasets/alessandrasala79/ai-vs-human-generated-dataset\n",
            "Downloading ai-vs-human-generated-dataset.zip to ./ai-vs-human-generated-dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.76G/9.76G [06:24<00:00, 27.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0c913c3a",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-05T09:12:43.947246Z",
          "iopub.status.busy": "2025-02-05T09:12:43.946879Z",
          "iopub.status.idle": "2025-02-05T09:12:52.922718Z",
          "shell.execute_reply": "2025-02-05T09:12:52.921934Z"
        },
        "papermill": {
          "duration": 8.982354,
          "end_time": "2025-02-05T09:12:52.924452",
          "exception": false,
          "start_time": "2025-02-05T09:12:43.942098",
          "status": "completed"
        },
        "tags": [],
        "id": "0c913c3a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6edd8f6b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-05T09:12:52.932168Z",
          "iopub.status.busy": "2025-02-05T09:12:52.931705Z",
          "iopub.status.idle": "2025-02-05T09:12:53.106491Z",
          "shell.execute_reply": "2025-02-05T09:12:53.105451Z"
        },
        "papermill": {
          "duration": 0.18016,
          "end_time": "2025-02-05T09:12:53.108100",
          "exception": false,
          "start_time": "2025-02-05T09:12:52.927940",
          "status": "completed"
        },
        "tags": [],
        "id": "6edd8f6b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d48fd65-233b-4ead-9bf9-b674318ab762"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset shape: (79950, 3)\n",
            "Test dataset shape: (5540, 1)\n",
            "Train columns: Index(['id', 'label'], dtype='object')\n",
            "Test columns: Index(['id'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "# Define paths to dataset files\n",
        "path = '/content/ai-vs-human-generated-dataset'\n",
        "train_csv = '/content/detect-ai-vs-human-generated-images/train.csv'\n",
        "test_csv = '/content/detect-ai-vs-human-generated-images/test.csv'\n",
        "\n",
        "# Load the training and test datasets\n",
        "train = pd.read_csv(train_csv)\n",
        "test = pd.read_csv(test_csv)\n",
        "\n",
        "# Print dataset shapes\n",
        "print(f'Training dataset shape: {train.shape}')\n",
        "print(f'Test dataset shape: {test.shape}')\n",
        "\n",
        "# Preprocess column names for consistency\n",
        "train = train[['file_name', 'label']]\n",
        "train.columns = ['id', 'label']\n",
        "\n",
        "# Display columns for reference\n",
        "print(\"Train columns:\", train.columns)\n",
        "print(\"Test columns:\", test.columns)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the training data into training and validation sets (95% train, 5% validation)\n",
        "train_df, val_df = train_test_split(\n",
        "    train,\n",
        "    test_size=0.05,\n",
        "    random_state=42,\n",
        "    stratify=train['label']\n",
        ")\n",
        "\n",
        "# Print shapes of the splits\n",
        "print(f'Train shape: {train_df.shape}')\n",
        "print(f'Validation shape: {val_df.shape}')\n",
        "\n",
        "# Check class distribution in both sets\n",
        "print(\"\\nTrain class distribution:\")\n",
        "print(train_df['label'].value_counts(normalize=True))\n",
        "\n",
        "print(\"\\nValidation class distribution:\")\n",
        "print(val_df['label'].value_counts(normalize=True))"
      ],
      "metadata": {
        "id": "YB9OZB3nY-EY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e413977-d80d-461e-a1ec-104c2ed5a7ff"
      },
      "id": "YB9OZB3nY-EY",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape: (75952, 2)\n",
            "Validation shape: (3998, 2)\n",
            "\n",
            "Train class distribution:\n",
            "label\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n",
            "\n",
            "Validation class distribution:\n",
            "label\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training augmentations\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize(232),  # Resize to match ConvNeXt preprocessing\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Validation and Test transforms\n",
        "val_test_transforms = transforms.Compose([\n",
        "    transforms.Resize(232),  # Resize to 232 as per ConvNeXt documentation\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "AMYBOpL4ZWY_"
      },
      "id": "AMYBOpL4ZWY_",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class for training and validation\n",
        "class AIImageDataset(Dataset):\n",
        "    def __init__(self, dataframe, root_dir, transform=None):\n",
        "        self.dataframe = dataframe\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.root_dir, self.dataframe.iloc[idx, 0])\n",
        "        image = Image.open(img_name).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        label = self.dataframe.iloc[idx, 1]\n",
        "        return image, label\n",
        "\n",
        "# Dataset class for inference (validation and test)\n",
        "class TestAIImageDataset(Dataset):\n",
        "    def __init__(self, file_list, transform=None):\n",
        "        self.file_list = file_list\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.file_list[idx]\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        return img, os.path.basename(img_path)  # Return image and filename"
      ],
      "metadata": {
        "id": "a84LpT9_ecTO"
      },
      "id": "a84LpT9_ecTO",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = AIImageDataset(train_df, root_dir=path, transform=train_transforms)\n",
        "\n",
        "# For validation, create a list of file paths and store labels separately\n",
        "val_file_list = [os.path.join(path, fname) for fname in val_df['id']]\n",
        "val_labels = val_df['label'].values  # Store labels separately for later use\n",
        "val_dataset = TestAIImageDataset(file_list=val_file_list, transform=val_test_transforms)\n",
        "\n",
        "# For testing, create a list of file paths\n",
        "test_file_list = [os.path.join(path, fname) for fname in test['id']]\n",
        "test_dataset = TestAIImageDataset(file_list=test_file_list, transform=val_test_transforms)\n",
        "\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "YP1-SoSHefAo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b44cb81-d17b-499a-eaae-af45345bd824"
      },
      "id": "YP1-SoSHefAo",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset size: 75952\n",
            "Validation dataset size: 3998\n",
            "Test dataset size: 5540\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pretrained ConvNeXt Base model\n",
        "model = models.convnext_base(weights=\"DEFAULT\")\n",
        "\n",
        "# Freeze all layers initially\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze the last two stages\n",
        "for param in model.features[-2:].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Replace the classifier head with a custom one\n",
        "model.classifier = nn.Sequential(\n",
        "    nn.AdaptiveAvgPool2d((1, 1)),  # Global average pooling\n",
        "    nn.Flatten(),                  # Flatten the tensor\n",
        "    nn.BatchNorm1d(1024),          # Add BatchNorm here\n",
        "    nn.Linear(1024, 512),          # First fully connected layer\n",
        "    nn.ReLU(),                     # Activation function\n",
        "    nn.Dropout(0.4),               # Dropout for regularization\n",
        "    nn.Linear(512, 2)              # Output layer (binary classification)\n",
        ")\n",
        "\n",
        "# Move the model to gpu\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and learning rate scheduler\n",
        "optimizer = torch.optim.AdamW([\n",
        "    {'params': model.features[-2:].parameters(), 'lr': 1e-5},  # Lower LR for backbone\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-4}      # Higher LR for classifier\n",
        "])\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=5, gamma=0.7)"
      ],
      "metadata": {
        "id": "4LTWHu2HejMi"
      },
      "id": "4LTWHu2HejMi",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training Start\")"
      ],
      "metadata": {
        "id": "odVSWQSaiaHD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c80233f1-0428-4fa2-bf8e-09f9fc38f87f"
      },
      "id": "odVSWQSaiaHD",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Start\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training Loop\n",
        "epochs = 12\n",
        "\n",
        "train_losses, train_accuracies, val_losses, val_accuracies, val_f1s = [], [], [], [], []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # -- Training --\n",
        "    model.train()\n",
        "    epoch_loss = 0.0\n",
        "    epoch_accuracy = 0.0\n",
        "\n",
        "    for data, label in tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\"):\n",
        "        data, label = data.to(device), label.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        preds = output.argmax(dim=1)\n",
        "        acc = (preds == label).float().mean().item()\n",
        "        epoch_accuracy += acc\n",
        "\n",
        "    epoch_loss /= len(train_loader)\n",
        "    epoch_accuracy /= len(train_loader)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_accuracies.append(epoch_accuracy)\n",
        "\n",
        "    # -- Validation --\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_pred_classes = []  # To store predictions\n",
        "    val_labels_list = []   # To store true labels\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (data, _) in enumerate(tqdm(val_loader, desc=f\"Validation Epoch {epoch+1}\")):\n",
        "            data = data.to(device)\n",
        "            output = model(data)\n",
        "\n",
        "            # Get true labels from val_df\n",
        "            batch_labels = val_labels[i * val_loader.batch_size : (i + 1) * val_loader.batch_size]\n",
        "            batch_labels = torch.tensor(batch_labels, device=device)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(output, batch_labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Compute predictions and accuracy\n",
        "            preds = output.argmax(dim=1)\n",
        "            acc = (preds == batch_labels).float().mean().item()\n",
        "            val_acc += acc\n",
        "\n",
        "            # Store predictions and true labels\n",
        "            val_pred_classes.extend(preds.cpu().numpy())\n",
        "            val_labels_list.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "    # Compute average validation metrics\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc /= len(val_loader)\n",
        "    val_f1 = f1_score(val_labels_list, val_pred_classes, average='binary')  # Binary classification\n",
        "\n",
        "    # Append metrics\n",
        "    val_losses.append(val_loss)\n",
        "    val_accuracies.append(val_acc)\n",
        "    val_f1s.append(val_f1)\n",
        "\n",
        "    print(\n",
        "        f\"Epoch [{epoch+1}/{epochs}] \"\n",
        "        f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_accuracy:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | Val F1: {val_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Step the learning rate scheduler\n",
        "    scheduler.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Y1DxFH1ib6_",
        "outputId": "b497606b-b5a3-4b16-b906-072c63c1cf78"
      },
      "id": "6Y1DxFH1ib6_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training Epoch 1:   1%|          | 22/2374 [08:25<14:42:29, 22.51s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate predictions and logits for the test set\n",
        "model.eval()\n",
        "test_logits = []  # To store logits\n",
        "test_pred_classes = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, _ in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n",
        "        data = data.to(device)\n",
        "        output = model(data)  # Raw logits (before softmax)\n",
        "\n",
        "        # Save logits\n",
        "        test_logits.extend(output.cpu().numpy())  # Store raw logits\n",
        "\n",
        "        # Get predicted class (0 or 1)\n",
        "        preds = output.argmax(dim=1)\n",
        "        test_pred_classes.extend(preds.cpu().numpy())\n",
        "\n",
        "# Convert logits to a DataFrame\n",
        "logits_df = pd.DataFrame(test_logits, columns=['logit_class_0', 'logit_class_1'])\n",
        "logits_df['id'] = test['id'].values  # Add image IDs for reference\n",
        "\n",
        "# Save logits to a CSV file\n",
        "logits_df.to_csv('test_logits.csv', index=False)\n",
        "\n",
        "# Add predictions to the test DataFrame\n",
        "test['label'] = test_pred_classes\n",
        "test[['id', 'label']].to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"Test logits saved to 'test_logits.csv'\")\n",
        "print(\"Test predictions saved to 'submission.csv'\")"
      ],
      "metadata": {
        "id": "8U-dupGAmGWE"
      },
      "id": "8U-dupGAmGWE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Generate predictions for the test set\n",
        "# model.eval()\n",
        "# test_pred_classes = []\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     for data, _ in tqdm(test_loader, desc=\"Generating Test Predictions\"):\n",
        "#         data = data.to(device)\n",
        "#         output = model(data)\n",
        "#         preds = output.argmax(dim=1)  # Get predicted class (0 or 1)\n",
        "#         test_pred_classes.extend(preds.cpu().numpy())\n",
        "\n",
        "# # Add predictions to the test DataFrame\n",
        "# test['label'] = test_pred_classes\n",
        "\n",
        "# # Save predictions to a CSV file\n",
        "# test[['id', 'label']].to_csv('submission.csv', index=False)\n",
        "# print(\"Test predictions saved to 'submission.csv'\")"
      ],
      "metadata": {
        "id": "JK9f6nPSmLz4"
      },
      "id": "JK9f6nPSmLz4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.read_csv('submission.csv')['label'].value_counts()"
      ],
      "metadata": {
        "id": "q5nzrHDfmNgb"
      },
      "id": "q5nzrHDfmNgb",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "databundleVersionId": 10884264,
          "sourceId": 91198,
          "sourceType": "competition"
        },
        {
          "datasetId": 6412205,
          "sourceId": 10550636,
          "sourceType": "datasetVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 228757,
          "modelInstanceId": 207022,
          "sourceId": 242370,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30840,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 9769.724299,
      "end_time": "2025-02-05T11:55:30.995749",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2025-02-05T09:12:41.271450",
      "version": "2.6.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}